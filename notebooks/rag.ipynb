{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Landmark RAG Pipeline\n",
        "\n",
        "Prepare Wikipedia content for Retrieval Augmented Generation:\n",
        "1. Load landmark Wikipedia data\n",
        "2. Chunk articles into retrievable segments\n",
        "3. Compute embeddings\n",
        "4. Test retrieval + generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('../src/rag/wiki-context.csv')\n",
        "print(f\"Loaded {len(df)} landmarks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_sentences(text):\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
        "    return [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "\n",
        "def chunk_text(text, tokenizer, max_tokens=400, overlap_tokens=50):\n",
        "    if not text or pd.isna(text):\n",
        "        return []\n",
        "    \n",
        "    sentences = split_sentences(text)\n",
        "    if not sentences:\n",
        "        return [text] if text.strip() else []\n",
        "    \n",
        "    chunks = []\n",
        "    current = []\n",
        "    current_len = 0\n",
        "    \n",
        "    for sent in sentences:\n",
        "        sent_len = len(tokenizer.encode(sent, add_special_tokens=False))\n",
        "        \n",
        "        if sent_len > max_tokens:\n",
        "            if current:\n",
        "                chunks.append(' '.join(current))\n",
        "                current, current_len = [], 0\n",
        "            chunks.append(sent)\n",
        "            continue\n",
        "        \n",
        "        if current_len + sent_len > max_tokens:\n",
        "            chunks.append(' '.join(current))\n",
        "            \n",
        "            overlap = []\n",
        "            overlap_len = 0\n",
        "            for s in reversed(current):\n",
        "                s_len = len(tokenizer.encode(s, add_special_tokens=False))\n",
        "                if overlap_len + s_len <= overlap_tokens:\n",
        "                    overlap.insert(0, s)\n",
        "                    overlap_len += s_len\n",
        "                else:\n",
        "                    break\n",
        "            current, current_len = overlap, overlap_len\n",
        "        \n",
        "        current.append(sent)\n",
        "        current_len += sent_len\n",
        "    \n",
        "    if current:\n",
        "        chunks.append(' '.join(current))\n",
        "    \n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "chunks = []\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    if pd.isna(row['page_content']):\n",
        "        continue\n",
        "    \n",
        "    for i, text in enumerate(chunk_text(row['page_content'], tokenizer)):\n",
        "        chunks.append({\n",
        "            'landmark_name': row['landmark_name'],\n",
        "            'wiki_url': row['wiki_url'],\n",
        "            'chunk_index': i,\n",
        "            'text': text\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").to(device)\n",
        "model.eval()\n",
        "\n",
        "def compute_embeddings(texts, batch_size=32):\n",
        "    embeddings = []\n",
        "    for i in tqdm(range(0, len(texts), batch_size)):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(**inputs).last_hidden_state.mean(dim=1)\n",
        "        embeddings.append(output.cpu().numpy())\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "chunk_texts = [c['text'] for c in chunks]\n",
        "embeddings = compute_embeddings(chunk_texts)\n",
        "print(f\"Embeddings shape: {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve(query, landmark_name, top_k=3):\n",
        "    indices = []\n",
        "    for i, c in enumerate(chunks):\n",
        "        if c['landmark_name'] == landmark_name:\n",
        "            indices.append(i)\n",
        "    if not indices:\n",
        "        return []\n",
        "    \n",
        "    landmark_chunks = [chunks[i] for i in indices]\n",
        "    landmark_embeddings = embeddings[indices]\n",
        "    \n",
        "    query_emb = compute_embeddings([query])[0]\n",
        "    \n",
        "    sims = np.dot(landmark_embeddings, query_emb) / (\n",
        "        np.linalg.norm(landmark_embeddings, axis=1) * np.linalg.norm(query_emb)\n",
        "    )\n",
        "    \n",
        "    top_idx = np.argsort(sims)[-top_k:][::-1]\n",
        "    return [(landmark_chunks[i], sims[i]) for i in top_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gen_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"cuda\"\n",
        ")\n",
        "gen_tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B-Instruct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def answer(question, landmark_name, top_k=3):\n",
        "    results = retrieve(question, landmark_name, top_k)\n",
        "    context = \"\\n\\n\".join([c['text'] for c, _ in results])\n",
        "    \n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Answer in 1-2 sentences using only the provided context.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nQuestion: {question}\"}\n",
        "    ]\n",
        "    \n",
        "    prompt = gen_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = gen_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = gen_model.generate(**inputs, max_new_tokens=100, pad_token_id=gen_tokenizer.eos_token_id)\n",
        "    \n",
        "    return gen_tokenizer.decode(outputs[0, inputs['input_ids'].shape[-1]:], skip_special_tokens=True).strip()\n",
        "\n",
        "\n",
        "test_cases = [\n",
        "    (\"What is the Lone Cypress?\", \"17-Mile-Drive\"),\n",
        "    (\"How long is the underground river?\", \"Puerto Princesa Undeground River\"),\n",
        "    (\"What are trulli houses made of?\", \"Alberobello's Trulli\"),\n",
        "]\n",
        "\n",
        "for q, landmark in test_cases:\n",
        "    print(f\"{landmark}: {answer(q, landmark)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"../src/rag/landmark_chunks.json\", \"w\") as f:\n",
        "    json.dump(chunks, f, indent=2)\n",
        "\n",
        "np.save(\"../src/rag/chunk_embeddings.npy\", embeddings)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
