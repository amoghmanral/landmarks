{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from peft import PeftModel\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DATA_DIR = Path(\"../data\")\n",
    "BASE = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(BASE)\n",
    "base_model = CLIPModel.from_pretrained(BASE).to(DEVICE).eval()\n",
    "lora_model = PeftModel.from_pretrained(CLIPModel.from_pretrained(BASE), \"../models/clip_lora/best_model\").to(DEVICE).eval()\n",
    "\n",
    "test_df = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "valid_paths, valid_landmarks = [], []\n",
    "for _, row in test_df.iterrows():\n",
    "    p = DATA_DIR / \"images\" / row[\"image_path\"]\n",
    "    if p.exists():\n",
    "        valid_paths.append(p)\n",
    "        valid_landmarks.append(row[\"landmark_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 38/176 [00:32<01:52,  1.22it/s]/Users/amoghmanral/Desktop/landmarks_new/.venv/lib/python3.13/site-packages/PIL/Image.py:1039: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 176/176 [02:20<00:00,  1.25it/s]\n",
      "100%|██████████| 176/176 [02:20<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "def embed_images(model, is_lora):\n",
    "    embeds = []\n",
    "    for i in tqdm(range(0, len(valid_paths), 32)):\n",
    "        imgs = [Image.open(p).convert(\"RGB\") for p in valid_paths[i:i+32]]\n",
    "        inputs = {k: v.to(DEVICE) for k, v in processor(images=imgs, return_tensors=\"pt\").items()}\n",
    "        with torch.no_grad():\n",
    "            if is_lora:\n",
    "                e = model.base_model.get_image_features(pixel_values=inputs[\"pixel_values\"])\n",
    "            else:\n",
    "                e = model.get_image_features(**inputs)\n",
    "            embeds.append((e / e.norm(dim=-1, keepdim=True)).cpu())\n",
    "    return torch.cat(embeds)\n",
    "\n",
    "base_embeds = embed_images(base_model, False)\n",
    "lora_embeds = embed_images(lora_model, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:06<00:00, 74.99it/s]\n",
      "100%|██████████| 500/500 [00:07<00:00, 69.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base CLIP:\n",
      "  Top-1: 30.8%, Top-5: 57.0%, Top-10: 69.0%\n",
      "LoRA CLIP:\n",
      "  Top-1: 33.2%, Top-5: 61.2%, Top-10: 70.6%\n",
      "Improvement:\n",
      "  Top-1: +2.400000000000002%, Top-5: +4.200000000000003%, Top-10: +1.5999999999999943%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, img_embeds, is_lora, n=500):\n",
    "    samples = test_df[test_df[\"landmark_name\"].isin(set(valid_landmarks))].sample(n=n, random_state=42)\n",
    "    hits = {1: 0, 5: 0, 10: 0}\n",
    "    \n",
    "    for _, row in tqdm(samples.iterrows(), total=n):\n",
    "        inputs = {k: v.to(DEVICE) for k, v in processor(text=[row[\"description\"]], return_tensors=\"pt\", padding=True, truncation=True).items()}\n",
    "        with torch.no_grad():\n",
    "            if is_lora:\n",
    "                t = model.base_model.get_text_features(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
    "            else:\n",
    "                t = model.get_text_features(**inputs)\n",
    "            t = t / t.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        sims = (t.cpu() @ img_embeds.T).squeeze()\n",
    "        \n",
    "        scores = {}\n",
    "        for idx, s in enumerate(sims):\n",
    "            scores.setdefault(valid_landmarks[idx], []).append(s.item())\n",
    "        \n",
    "        landmark_avg = {lm: sum(v)/len(v) for lm, v in scores.items()}\n",
    "        ranked = sorted(landmark_avg, key=landmark_avg.get, reverse=True)\n",
    "        \n",
    "        for k in hits:\n",
    "            if row[\"landmark_name\"] in ranked[:k]:\n",
    "                hits[k] += 1\n",
    "    \n",
    "    return {k: round(100 * v / n, 1) for k, v in hits.items()}\n",
    "\n",
    "base_res = evaluate(base_model, base_embeds, False)\n",
    "lora_res = evaluate(lora_model, lora_embeds, True)\n",
    "\n",
    "print(\"Base CLIP:\")\n",
    "print(f\"  Top-1: {base_res[1]}%, Top-5: {base_res[5]}%, Top-10: {base_res[10]}%\")\n",
    "print(\"LoRA CLIP:\")\n",
    "print(f\"  Top-1: {lora_res[1]}%, Top-5: {lora_res[5]}%, Top-10: {lora_res[10]}%\")\n",
    "print(\"Improvement:\")\n",
    "print(f\"  Top-1: +{lora_res[1] - base_res[1]}%, Top-5: +{lora_res[5] - base_res[5]}%, Top-10: +{lora_res[10] - base_res[10]}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
