{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ienoahBYoBt9",
        "outputId": "dda72a28-5424-40cb-f63d-c13340f8a1c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "replace ./__MACOSX/._landmark_images? [y]es, [n]o, [A]ll, [N]one, [r]ename: Files ready\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive and load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unzip images (comment out after first run)\n",
        "!unzip -q \"/content/drive/MyDrive/csproj/landmark_images.zip\" -d .\n",
        "\n",
        "# Fix nested folder structure if needed\n",
        "!if [ -d \"landmark_images/landmark_images\" ]; then mv landmark_images/landmark_images/* landmark_images/ && rm -rf landmark_images/__MACOSX landmark_images/landmark_images; fi\n",
        "\n",
        "# Copy CSVs\n",
        "!cp \"/content/drive/MyDrive/csproj/train.csv\" .\n",
        "!cp \"/content/drive/MyDrive/csproj/val.csv\" .\n",
        "!cp \"/content/drive/MyDrive/csproj/test.csv\" .\n",
        "\n",
        "\n",
        "print(\"Files ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWKtt9WaXgLG"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0rnneoOXhXH",
        "outputId": "d7f53bc1-d185-44ec-ff29-bfefcf8d275e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports successful\n",
            "PyTorch version: 2.9.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import random\n",
        "import unicodedata\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.amp import autocast, GradScaler\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "print(\"Imports successful\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCX9DSSAZytK"
      },
      "source": [
        "##Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcUb4xZoZzgu",
        "outputId": "65f7e0d7-ffd5-4447-b8ee-d9f4c148937f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "OUTPUT_DIR = \"clip_lora\"\n",
        "CHECKPOINT_DIR = \"checkpoints_lora\"\n",
        "\n",
        "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "ACCUMULATION_STEPS = 1\n",
        "LEARNING_RATE = 2e-4\n",
        "NUM_EPOCHS = 10\n",
        "WEIGHT_DECAY = 1e-2\n",
        "\n",
        "# LoRA settings\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "LORA_DROPOUT = 0.05\n",
        "WEIGHT_DECAY = 1e-3\n",
        "\n",
        "# Data sampling (per landmark)\n",
        "TRAIN_IMAGES_PER_LANDMARK = 11\n",
        "VAL_IMAGES_PER_LANDMARK = 3\n",
        "TEST_IMAGES_PER_LANDMARK = 2\n",
        "\n",
        "# Regularization\n",
        "EARLY_STOPPING_PATIENCE = 3\n",
        "GRADIENT_CLIP_MAX_NORM = 1.0\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "Path(OUTPUT_DIR).mkdir(exist_ok=True)\n",
        "Path(CHECKPOINT_DIR).mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wkF5sz1YLvX"
      },
      "source": [
        "## Util"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n336sQ6OYK9O",
        "outputId": "120b30c6-59ef-4035-cff2-d65a1708826c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random seed set to 42\n"
          ]
        }
      ],
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def normalize_path(path):\n",
        "    return unicodedata.normalize('NFD', path)\n",
        "\n",
        "def load_csv_safe(filepath):\n",
        "    rows = []\n",
        "    with open(filepath, 'r') as f:\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader)\n",
        "        for row in reader:\n",
        "            rows.append(row)\n",
        "    return pd.DataFrame(rows, columns=header)\n",
        "\n",
        "set_seed(SEED)\n",
        "print(f\"Random seed set to {SEED}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9JH7UexY1zT"
      },
      "source": [
        "## Sample Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkRdB5Z3Y3W6",
        "outputId": "b9defe8f-fcb7-4892-b405-0e7795ab8539"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading CSVs...\n",
            "Original sizes:\n",
            "Train: 262,640 pairs, 26,264 images\n",
            "Val: 5,628 pairs, 5,628 images\n",
            "Test: 5,628 pairs, 5,628 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2823821704.py:41: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  train_df = train_df.groupby('image_path').apply(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "After sampling:\n",
            "Train: 51,590 pairs, 10,318 images, 938 landmarks\n",
            "Val: 2,814 pairs, 2,814 images, 938 landmarks\n",
            "Test: 1,876 pairs, 1,876 images, 938 landmarks\n",
            "\n",
            "Saved sampled CSVs\n"
          ]
        }
      ],
      "source": [
        "def sample_images_per_landmark(df, images_per_landmark, seed=42):\n",
        "    sampled_rows = []\n",
        "\n",
        "    for landmark, group in df.groupby('landmark_name'):\n",
        "        # Get unique images for this landmark\n",
        "        unique_images = group.drop_duplicates('image_path')\n",
        "\n",
        "        # Sample up to N images\n",
        "        n_sample = min(images_per_landmark, len(unique_images))\n",
        "        sampled_images = unique_images.sample(n=n_sample, random_state=seed)\n",
        "\n",
        "        # Keep all rows (captions) for these sampled images\n",
        "        sampled_image_paths = sampled_images['image_path'].tolist()\n",
        "        landmark_rows = group[group['image_path'].isin(sampled_image_paths)]\n",
        "        sampled_rows.append(landmark_rows)\n",
        "\n",
        "    return pd.concat(sampled_rows, ignore_index=True)\n",
        "\n",
        "# Load CSVs\n",
        "print(\"Loading CSVs...\")\n",
        "train_df = load_csv_safe('train.csv')\n",
        "val_df = load_csv_safe('val.csv')\n",
        "test_df = load_csv_safe('test.csv')\n",
        "\n",
        "# Normalize paths\n",
        "train_df['image_path'] = train_df['image_path'].apply(normalize_path)\n",
        "val_df['image_path'] = val_df['image_path'].apply(normalize_path)\n",
        "test_df['image_path'] = test_df['image_path'].apply(normalize_path)\n",
        "\n",
        "print(f\"Original sizes:\")\n",
        "print(f\"Train: {len(train_df):,} pairs, {train_df['image_path'].nunique():,} images\")\n",
        "print(f\"Val: {len(val_df):,} pairs, {val_df['image_path'].nunique():,} images\")\n",
        "print(f\"Test: {len(test_df):,} pairs, {test_df['image_path'].nunique():,} images\")\n",
        "\n",
        "# Sample images per landmark\n",
        "train_df = sample_images_per_landmark(train_df, TRAIN_IMAGES_PER_LANDMARK, seed=SEED)\n",
        "val_df = sample_images_per_landmark(val_df, VAL_IMAGES_PER_LANDMARK, seed=SEED)\n",
        "test_df = sample_images_per_landmark(test_df, TEST_IMAGES_PER_LANDMARK, seed=SEED)\n",
        "\n",
        "# Limit to 5 captions per image (train only)\n",
        "train_df = train_df.groupby('image_path').apply(\n",
        "    lambda x: x.sample(min(5, len(x)), random_state=SEED)\n",
        ").reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nAfter sampling:\")\n",
        "print(f\"Train: {len(train_df):,} pairs, {train_df['image_path'].nunique():,} images, {train_df['landmark_name'].nunique()} landmarks\")\n",
        "print(f\"Val: {len(val_df):,} pairs, {val_df['image_path'].nunique():,} images, {val_df['landmark_name'].nunique()} landmarks\")\n",
        "print(f\"Test: {len(test_df):,} pairs, {test_df['image_path'].nunique():,} images, {test_df['landmark_name'].nunique()} landmarks\")\n",
        "\n",
        "# Save sampled CSVs\n",
        "train_df.to_csv('train_sampled.csv', index=False)\n",
        "val_df.to_csv('val_sampled.csv', index=False)\n",
        "test_df.to_csv('test_sampled.csv', index=False)\n",
        "print(\"\\nSaved sampled CSVs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTVsI5MqY8R0"
      },
      "source": [
        "## Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKWJMDmpY8Cy",
        "outputId": "d5fa4d43-eed4-460e-aa9a-0d8379a7f74e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset class defined\n"
          ]
        }
      ],
      "source": [
        "class LandmarkDataset(Dataset):\n",
        "\n",
        "    def __init__(self, csv_path, processor):\n",
        "        self.df = pd.read_csv(csv_path)\n",
        "        self.processor = processor\n",
        "        self.resize = transforms.Resize((224, 224))\n",
        "\n",
        "        # Filter out missing images\n",
        "        before = len(self.df)\n",
        "        self.df = self.df[self.df['image_path'].apply(lambda p: Path(p).exists())]\n",
        "        after = len(self.df)\n",
        "\n",
        "        if before != after:\n",
        "            print(f\"  Warning: {before - after} missing images filtered out\")\n",
        "\n",
        "        print(f\"Loaded {len(self.df):,} pairs from {csv_path}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = Image.open(row['image_path']).convert('RGB')\n",
        "        image = self.resize(image)\n",
        "\n",
        "        return {\n",
        "            'image': image,\n",
        "            'text': row['description'],\n",
        "            'landmark_name': row['landmark_name']\n",
        "        }\n",
        "\n",
        "print(\"Dataset class defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jI22CsOXxEs"
      },
      "source": [
        "## Load Model with LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXoWu1FrXybN",
        "outputId": "5a914040-6dcb-4805-f853-c840848fbc16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading CLIP model...\n",
            "\n",
            "LoRA model loaded\n",
            "Trainable params: 3,932,160 (2.53%)\n",
            "Total params: 155,209,473\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Loading CLIP model...\")\n",
        "processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
        "base_model = CLIPModel.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Configure LoRA - target the attention layers\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\",  # Attention projections\n",
        "    ],\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Print trainable parameters\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\nLoRA model loaded\")\n",
        "print(f\"Trainable params: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
        "print(f\"Total params: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFyTTBAdXzyg"
      },
      "source": [
        "## Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmTjsKt5X0pI",
        "outputId": "92eb06ab-7a16-4570-8ca2-da74d2bf0150"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training functions defined\n"
          ]
        }
      ],
      "source": [
        "def contrastive_loss(image_embeds, text_embeds, logit_scale):\n",
        "    image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
        "    text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    logits_per_image = logit_scale * (image_embeds @ text_embeds.t())\n",
        "    logits_per_text = logits_per_image.t()\n",
        "\n",
        "    batch_size = len(image_embeds)\n",
        "    labels = torch.arange(batch_size, device=image_embeds.device)\n",
        "\n",
        "    loss_i2t = nn.functional.cross_entropy(logits_per_image, labels)\n",
        "    loss_t2i = nn.functional.cross_entropy(logits_per_text, labels)\n",
        "\n",
        "    return (loss_i2t + loss_t2i) / 2\n",
        "\n",
        "\n",
        "def train_epoch_lora(model, dataloader, optimizer, scheduler, scaler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=\"Training\")\n",
        "    for batch_idx, batch in enumerate(pbar):\n",
        "        pixel_values = batch['pixel_values'].to(device)\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        with autocast('cuda', enabled=(device == 'cuda')):\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                pixel_values=pixel_values,\n",
        "                return_loss=False\n",
        "            )\n",
        "            image_embeds = outputs.image_embeds\n",
        "            text_embeds = outputs.text_embeds\n",
        "            logit_scale = model.base_model.logit_scale.exp()\n",
        "\n",
        "            loss = contrastive_loss(image_embeds, text_embeds, logit_scale)\n",
        "            loss = loss / ACCUMULATION_STEPS\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (batch_idx + 1) % ACCUMULATION_STEPS == 0 or (batch_idx + 1) == len(dataloader):\n",
        "            scaler.unscale_(optimizer)\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP_MAX_NORM)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * ACCUMULATION_STEPS\n",
        "        pbar.set_postfix({'loss': f'{loss.item() * ACCUMULATION_STEPS:.4f}'})\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "\n",
        "def evaluate_lora(model, dataloader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                pixel_values=pixel_values,\n",
        "                return_loss=False\n",
        "            )\n",
        "            image_embeds = outputs.image_embeds\n",
        "            text_embeds = outputs.text_embeds\n",
        "            logit_scale = model.base_model.logit_scale.exp()\n",
        "\n",
        "            loss = contrastive_loss(image_embeds, text_embeds, logit_scale)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
        "            text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
        "            similarity = logit_scale * (image_embeds @ text_embeds.t())\n",
        "            predictions = similarity.argmax(dim=1)\n",
        "            labels = torch.arange(len(predictions), device=device)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += len(predictions)\n",
        "\n",
        "    return total_loss / len(dataloader), correct / total\n",
        "\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=2):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None:\n",
        "            self.best_loss = val_loss\n",
        "        elif val_loss >= self.best_loss:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                return True\n",
        "        else:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        return False\n",
        "\n",
        "print(\"Training functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jceRvnAaaLgQ"
      },
      "source": [
        "## Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sN0u2ywGaI-p",
        "outputId": "04192e26-a2b6-4397-b670-f993703d0431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating datasets...\n",
            "Loaded 51,590 pairs from train_sampled.csv\n",
            "Loaded 2,814 pairs from val_sampled.csv\n",
            "\n",
            "DataLoaders created\n",
            "Train batches: 404\n",
            "Val batches: 22\n"
          ]
        }
      ],
      "source": [
        "def collate_fn(batch):\n",
        "    images = [item['image'] for item in batch]\n",
        "    texts = [item['text'] for item in batch]\n",
        "\n",
        "    inputs = processor(\n",
        "        text=texts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        'pixel_values': inputs['pixel_values'],\n",
        "        'input_ids': inputs['input_ids'],\n",
        "        'attention_mask': inputs['attention_mask'],\n",
        "    }\n",
        "\n",
        "print(\"Creating datasets...\")\n",
        "train_dataset = LandmarkDataset('train_sampled.csv', processor)\n",
        "val_dataset = LandmarkDataset('val_sampled.csv', processor)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=8,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=(DEVICE == 'cuda')\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        "    collate_fn=collate_fn,\n",
        "    pin_memory=(DEVICE == 'cuda')\n",
        ")\n",
        "\n",
        "print(f\"\\nDataLoaders created\")\n",
        "print(f\"Train batches: {len(train_loader)}\")\n",
        "print(f\"Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uc93PlbxX1OU"
      },
      "source": [
        "## Setup Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGpBBRjaX3lC",
        "outputId": "a8f78081-b8ec-4ce8-cbba-ed36396018af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training components initialized\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LEARNING_RATE,\n",
        "    weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=len(train_loader) * NUM_EPOCHS\n",
        ")\n",
        "\n",
        "scaler = GradScaler('cuda', enabled=(DEVICE == 'cuda'))\n",
        "early_stopping = EarlyStopping(patience=EARLY_STOPPING_PATIENCE)\n",
        "\n",
        "print(\"Training components initialized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTWqfAZgX4Eq"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv-W0cm-X6QY",
        "outputId": "38fda193-e61c-41a7-a9d3-c0472a92f24e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Starting LoRA training...\n",
            "============================================================\n",
            "\n",
            "Epoch 1/10\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   4%|▍         | 17/404 [00:14<02:43,  2.37it/s, loss=2.1353]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  10%|█         | 41/404 [00:28<01:38,  3.67it/s, loss=2.0055]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  14%|█▍        | 57/404 [00:37<02:05,  2.77it/s, loss=1.9272]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  19%|█▉        | 78/404 [00:51<01:48,  3.00it/s, loss=1.6247]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  21%|██▏       | 86/404 [00:56<01:59,  2.67it/s, loss=1.9546]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  30%|███       | 122/404 [01:17<03:43,  1.26it/s, loss=1.7457]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  39%|███▉      | 159/404 [01:39<02:25,  1.68it/s, loss=1.4829]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  41%|████      | 166/404 [01:44<02:02,  1.94it/s, loss=1.2435]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  54%|█████▍    | 220/404 [02:17<02:50,  1.08it/s, loss=1.2490]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  56%|█████▋    | 228/404 [02:22<00:55,  3.19it/s, loss=1.2119]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  58%|█████▊    | 235/404 [02:26<00:52,  3.21it/s, loss=1.0649]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  94%|█████████▍| 379/404 [03:55<00:20,  1.21it/s, loss=0.7564]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 404/404 [04:10<00:00,  1.61it/s, loss=0.0499]\n",
            "Evaluating:  45%|████▌     | 10/22 [00:09<00:10,  1.16it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Evaluating: 100%|██████████| 22/22 [00:17<00:00,  1.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results:\n",
            "  Train Loss: 1.3672\n",
            "  Val Loss: 3.1319\n",
            "  Val Accuracy: 0.2697\n",
            "  ✓ New best model saved!\n",
            "\n",
            "Epoch 2/10\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining:   0%|          | 0/404 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:   2%|▏         | 8/404 [00:09<02:35,  2.54it/s, loss=0.6474]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:   6%|▌         | 24/404 [00:19<01:57,  3.24it/s, loss=0.4461]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:   8%|▊         | 32/404 [00:24<01:39,  3.75it/s, loss=0.5514]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  20%|█▉        | 80/404 [00:55<01:26,  3.76it/s, loss=0.5085]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  30%|██▉       | 120/404 [01:19<01:20,  3.55it/s, loss=0.5951]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  34%|███▎      | 136/404 [01:28<01:15,  3.55it/s, loss=0.5374]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  39%|███▉      | 157/404 [01:42<02:05,  1.97it/s, loss=0.4615]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  42%|████▏     | 168/404 [01:48<01:40,  2.34it/s, loss=0.3913]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  43%|████▎     | 174/404 [01:52<02:16,  1.68it/s, loss=0.5397]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 404/404 [04:09<00:00,  1.62it/s, loss=0.0000]\n",
            "Evaluating:  50%|█████     | 11/22 [00:10<00:06,  1.68it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Evaluating: 100%|██████████| 22/22 [00:15<00:00,  1.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results:\n",
            "  Train Loss: 0.4830\n",
            "  Val Loss: 3.6525\n",
            "  Val Accuracy: 0.2537\n",
            "\n",
            "Epoch 3/10\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining:   0%|          | 0/404 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:   2%|▏         | 10/404 [00:09<02:08,  3.06it/s, loss=0.2880]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:   4%|▍         | 17/404 [00:13<01:58,  3.27it/s, loss=0.2269]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:   4%|▍         | 18/404 [00:13<01:44,  3.69it/s, loss=0.2433]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:   5%|▍         | 20/404 [00:16<04:47,  1.33it/s, loss=0.2952]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  12%|█▏        | 50/404 [00:33<01:39,  3.56it/s, loss=0.2387]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  35%|███▌      | 143/404 [01:30<01:52,  2.33it/s, loss=0.3224]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  40%|████      | 162/404 [01:40<01:08,  3.53it/s, loss=0.4349]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  57%|█████▋    | 231/404 [02:22<01:20,  2.15it/s, loss=0.3368]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  63%|██████▎   | 255/404 [02:38<01:07,  2.20it/s, loss=0.3544]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 404/404 [04:11<00:00,  1.60it/s, loss=0.0001]\n",
            "Evaluating:  64%|██████▎   | 14/22 [00:11<00:03,  2.28it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Evaluating: 100%|██████████| 22/22 [00:15<00:00,  1.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results:\n",
            "  Train Loss: 0.3062\n",
            "  Val Loss: 3.7981\n",
            "  Val Accuracy: 0.2576\n",
            "\n",
            "Epoch 4/10\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rTraining:   0%|          | 0/404 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  15%|█▌        | 62/404 [00:41<01:56,  2.94it/s, loss=0.3161]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  18%|█▊        | 72/404 [00:46<02:19,  2.38it/s, loss=0.3350]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  25%|██▍       | 100/404 [01:04<03:39,  1.38it/s, loss=0.2560]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  30%|███       | 122/404 [01:16<01:17,  3.64it/s, loss=0.1655]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  31%|███       | 124/404 [01:16<01:05,  4.29it/s, loss=0.3447]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  39%|███▉      | 159/404 [01:39<02:02,  1.99it/s, loss=0.3027]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  40%|████      | 163/404 [01:41<01:21,  2.95it/s, loss=0.2629]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  41%|████      | 165/404 [01:41<01:04,  3.69it/s, loss=0.2474]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  49%|████▉     | 197/404 [02:01<00:53,  3.90it/s, loss=0.3759]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training:  51%|█████     | 205/404 [02:07<01:00,  3.27it/s, loss=0.2556]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  58%|█████▊    | 234/404 [02:25<01:22,  2.07it/s, loss=0.1972]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  84%|████████▍ | 341/404 [03:30<00:17,  3.53it/s, loss=0.2295]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Training:  98%|█████████▊| 395/404 [04:01<00:02,  3.48it/s, loss=0.2549]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:3452: DecompressionBombWarning: Image size (130379776 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 404/404 [04:05<00:00,  1.65it/s, loss=0.0026]\n",
            "Evaluating:  64%|██████▎   | 14/22 [00:10<00:02,  3.17it/s]/usr/local/lib/python3.12/dist-packages/PIL/Image.py:1047: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  warnings.warn(\n",
            "Evaluating: 100%|██████████| 22/22 [00:15<00:00,  1.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Results:\n",
            "  Train Loss: 0.2445\n",
            "  Val Loss: 3.8597\n",
            "  Val Accuracy: 0.2665\n",
            "\n",
            "============================================================\n",
            "Early stopping triggered at epoch 4\n",
            "============================================================\n",
            "\n",
            "============================================================\n",
            "Training complete!\n",
            "Best validation loss: 3.1319\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Starting LoRA training...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "history = {'train_loss': [], 'val_loss': [], 'val_accuracy': []}\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{NUM_EPOCHS}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    train_loss = train_epoch_lora(model, train_loader, optimizer, scheduler, scaler, DEVICE)\n",
        "    val_loss, val_accuracy = evaluate_lora(model, val_loader, DEVICE)\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_accuracy'].append(val_accuracy)\n",
        "\n",
        "    print(f\"\\nResults:\")\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"  Val Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    # Save checkpoint\n",
        "    checkpoint = {\n",
        "        'epoch': epoch + 1,\n",
        "        'val_loss': val_loss,\n",
        "        'val_accuracy': val_accuracy,\n",
        "    }\n",
        "    model.save_pretrained(f\"{CHECKPOINT_DIR}/checkpoint_epoch_{epoch + 1}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        model.save_pretrained(f\"{OUTPUT_DIR}/best_model\")\n",
        "        print(f\"  ✓ New best model saved!\")\n",
        "\n",
        "    if early_stopping(val_loss):\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        break\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Training complete!\")\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGcbAeUlYHAh"
      },
      "source": [
        "## Save to drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOYjgQ73YIJz",
        "outputId": "ecb2a7f8-a8f2-447f-a9e6-a0501c8172ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ LoRA model saved to Google Drive\n"
          ]
        }
      ],
      "source": [
        "!cp -r {OUTPUT_DIR} \"/content/drive/MyDrive/csproj/\"\n",
        "!cp -r {CHECKPOINT_DIR} \"/content/drive/MyDrive/csproj/\"\n",
        "print(\"✓ LoRA model saved to Google Drive\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
